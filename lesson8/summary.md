现在我们来复盘一下cuda的组织结构，cuda本质上是GPU。GPU上面连着显卡。



计算机内存分为 cpu ram，GPU ram，GPU ram就是我们说的显存。比如rtx 2060的显存就是6GB。

显存里的数据就是device中文叫设备。主机上的设备叫host，就是放在一般计算机的ram中的，叫主存。其实还有一块东西，交外存，就是看的32G+1TB。那个32G就是主存 1TB就是外存。我们这里在host说的就是主存32GB这个。



现在回到显卡，显卡是由GPU+显存构成的。 显存就是我们说device_memory。可以理解为，可以理解显卡就是一个小型的电脑，他也有CPU和内存。其中和正常电脑对比，CPU- GPU, 主存 - 显存。显卡没有辅存这个观念。

我们先将一下GPU运算的核心思路

1. 将要算的东西放到GPU上
2. 然后利用GPU并行计算的思路去加速计算。
3. 然后在HostToDevice 拷贝回CPU中使用。 这样权责分离 GPU就只负责那块大规模的运算

GPU具体怎么加速的呢？？

他的并行思路就是将for循环这种顺序执行的东西，变成并发执行的。这样for循环单位时间内只能跑一条程序，



比如：

我想做把两个100000000长度的向量A和B相加，用CPU的同步思考方向来看，就是写一个for循环，0号加完加1号，1号加完加2号一直加到100000000-1号。就完成了这个操作。

但是GPU不一样，他是怎么想的呢？我可以把这些重复的操作给剥离出来，啥意思呢？

就是重复10000000次同样的加操作，我可以利用GPU的并行特点，开辟多个**线程** 让这些线程一起计算，这样同时算多个线程，就能**降维打击，加速计算了**



现在有了线程这个概念，线程具体是怎么组织的呢，若干个线程组成一个**线程块**block， 若干个线程块拼在一起组成了一个**框架** gird。

其中block_size 是一个dim3类型的，有三个维度，分别是x,y,z，表示这个线程块有三个维度。

block_size决定了线程块里线程的多少。gird_size决定了线程块的多少。

现在又有问题了**为什么要定义线程块呢？** 我把所有的线程都拿出来同时跑不就行了吗？

这种想法有两个缺陷。

1. GPU不是神，达不到这么快的并发量，受摩尔定律影响
2. 哪怕真的可以同时跑，这种设计思路也只适合计算加法这些，对别的没啥帮助

所以我们定义了线程块，在线程块内允许**共享内存 share_memory** 这样就可以发生一些交互，能够更加方便我们加速了。



具体线程块怎么执行呢？，这就引入了SM的概念：**SM是实际运行线程的**

GPU和核心运算单元是SM

比如RTX A6000 有84组SM

SM是**Streaming Multiprocessor** 是整个GPU的核心。SM里有寄存器，和共享内存。

其中对于一个核函数来说

```
__global void add(const double* x, const double *y , double* z)
{
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    
    z[i] = x[i]+y[i];
}

```

这里的i就是放在**寄存器**里的，比如，我们这里的RTX A6000，一个SM的总可用寄存器大小就是64KB，共享内存大小是128KB。

比如这里的一个线程用了一个int字节，是4字节也就是4B。 这样来说一个SM，最多放64*1024 ÷ 4 = 16384个线程。但是实际上还有更多限制。

比如这里64*1024 / 4的算法，是基于一种**将线程一个一个放进SM**的计算思路，但实际上不可能这样，因为这种**一个一个**one by one 的思想在并行计算中就是错误的。GPU是以一种**线程束 warp**  来组织的。也就一口气抓若干个线程，这里一般的GPU架构抓的都是**32**。  所以实际上，放进SM的线程个数必须是WARP 大小的整数倍，也就是**32的整数倍**。

但同时，SM中的线程也不是无穷的，**受限硬件限制**，一个SM中线程数最多128

所以寄存器的大小是影响SM占有率的一个关键。比如我一个线程用了 512B的寄存器，那么这个SM一共可以用64*1024  /512 = 128; 哦这说明SM的占有率被拉满了，很好我很喜欢。

但如果一个线程用了1024B的寄存器，那么这个SM一共可以用64*1024/1024 =64。 SM的占有率只有50%。

同样的，现在我的代码写的不好，一个线程用了500B(事实上也很容易发生，少开了一个double，两个int)的寄存器，那么我的SM一共可以用 64*1024/520 = 131 余360，也就是说一共可以放下131个线程。但是SM**是以线程束为根本的**！所以SM最多放131/32 = 4 余若干还是只能放4 * 32 =128个线程，就没有很好的利用掉寄存器。当然寄存器用不完也是可以的。



现在又有新问题了，线程中我可以在寄存器中开int类型变量，那么可不可以开数组呢？

答：当然可以，数组也放寄存器中，如果寄存器放满了，他会开在**本地内存 local_memory**中

，这个local_memory**是每个线程独有的**，他放在global_memory。我们前面说过 global_memory里的数据访问的特别慢，所以尽量减少对这里的访问。



ok现在我们讨论问SM及具体是怎么加速以及某些制约了。

加入一个程序的核心思路是

1. 加大并发量
2. 提高SM利用率，让更多的线程同时并发
3. 减少对global_memory的访问次数。

因此，GPU的内存架构，提供了share_memory这一概念，也就是线程块的作用就在此发挥了。



![1738594370186](D:\fpga\cuda\cudastep\lesson8\figure\1738594370186.png)

共享内存是CUDA加速的核心思路。

共享内存的特点就是就是同一个线程块里的线程对其的访问速度快







