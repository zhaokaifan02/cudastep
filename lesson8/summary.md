现在我们来复盘一下cuda的组织结构，cuda本质上是GPU。GPU上面连着显卡。



计算机内存分为 cpu ram，GPU ram，GPU ram就是我们说的显存。比如rtx 2060的显存就是6GB。

显存里的数据就是device中文叫设备。主机上的设备叫host，就是放在一般计算机的ram中的，叫主存。其实还有一块东西，交外存，就是看的32G+1TB。那个32G就是主存 1TB就是外存。我们这里在host说的就是主存32GB这个。



现在回到显卡，显卡是由GPU+显存构成的。 显存就是我们说device_memory。可以理解为，可以理解显卡就是一个小型的电脑，他也有CPU和内存。其中和正常电脑对比，CPU- GPU, 主存 - 显存。显卡没有辅存这个观念。

我们先将一下GPU运算的核心思路

1. 将要算的东西放到GPU上
2. 然后利用GPU并行计算的思路去加速计算。
3. 然后在HostToDevice 拷贝回CPU中使用。 这样权责分离 GPU就只负责那块大规模的运算

GPU具体怎么加速的呢？？

他的并行思路就是将for循环这种顺序执行的东西，变成并发执行的。这样for循环单位时间内只能跑一条程序，



比如：

我想做把两个100000000长度的向量A和B相加，用CPU的同步思考方向来看，就是写一个for循环，0号加完加1号，1号加完加2号一直加到100000000-1号。就完成了这个操作。

但是GPU不一样，他是怎么想的呢？我可以把这些重复的操作给剥离出来，啥意思呢？

就是重复10000000次同样的加操作，我可以利用GPU的并行特点，开辟多个**线程** 让这些线程一起计算，这样同时算多个线程，就能**降维打击，加速计算了**



现在有了线程这个概念，线程具体是怎么组织的呢，若干个线程组成一个**线程块**block， 若干个线程块拼在一起组成了一个**框架** gird。

其中block_size 是一个dim3类型的，有三个维度，分别是x,y,z，表示这个线程块有三个维度。

block_size决定了线程块里线程的多少。gird_size决定了线程块的多少。

现在又有问题了**为什么要定义线程块呢？** 我把所有的线程都拿出来同时跑不就行了吗？

这种想法有两个缺陷。

1. GPU不是神，达不到这么快的并发量，受摩尔定律影响
2. 哪怕真的可以同时跑，这种设计思路也只适合计算加法这些，对别的没啥帮助

所以我们定义了线程块，在线程块内允许**共享内存 share_memory** 这样就可以发生一些交互，能够更加方便我们加速了。



具体线程块怎么执行呢？，这就引入了SM的概念：**SM是实际运行线程的**

GPU和核心运算单元是SM

比如RTX A6000 有84组SM

SM是**Streaming Multiprocessor** 是整个GPU的核心。SM里有寄存器，和共享内存。

其中对于一个核函数来说

```
__global void add(const double* x, const double *y , double* z)
{
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    
    z[i] = x[i]+y[i];
}

```

这里的i就是放在**寄存器**里的，比如，我们这里的RTX A6000，一个SM的总可用寄存器大小就是64KB，共享内存大小是128KB。

比如这里的一个线程用了一个int字节，是4字节也就是4B。 这样来说一个SM，最多放64*1024 ÷ 4 = 16384个线程。但是实际上还有更多限制。

比如这里64*1024 / 4的算法，是基于一种**将线程一个一个放进SM**的计算思路，但实际上不可能这样，因为这种**一个一个**one by one 的思想在并行计算中就是错误的。GPU是以一种**线程束 warp**  来组织的。也就一口气抓若干个线程，这里一般的GPU架构抓的都是**32**。  所以实际上，放进SM的线程个数必须是WARP 大小的整数倍，也就是**32的整数倍**。

但同时，SM中的线程也不是无穷的，**受限硬件限制**，一个SM中线程数最多128

所以寄存器的大小是影响SM占有率的一个关键。比如我一个线程用了 512B的寄存器，那么这个SM一共可以用64*1024  /512 = 128; 哦这说明SM的占有率被拉满了，很好我很喜欢。

但如果一个线程用了1024B的寄存器，那么这个SM一共可以用64*1024/1024 =64。 SM的占有率只有50%。

同样的，现在我的代码写的不好，一个线程用了500B(事实上也很容易发生，少开了一个double，两个int)的寄存器，那么我的SM一共可以用 64*1024/520 = 131 余360，也就是说一共可以放下131个线程。但是SM**是以线程束为根本的**！所以SM最多放131/32 = 4 余若干还是只能放4 * 32 =128个线程，就没有很好的利用掉寄存器。当然寄存器用不完也是可以的。



现在又有新问题了，线程中我可以在寄存器中开int类型变量，那么可不可以开数组呢？

答：当然可以，数组也放寄存器中，如果寄存器放满了，他会开在**本地内存 local_memory**中

，这个local_memory**是每个线程独有的**，他放在global_memory。我们前面说过 global_memory里的数据访问的特别慢，所以尽量减少对这里的访问。



ok现在我们讨论问SM及具体是怎么加速以及某些制约了。

加入一个程序的核心思路是

1. 加大并发量
2. 提高SM利用率，让更多的线程同时并发
3. 减少对global_memory的访问次数。

因此，GPU的内存架构，提供了share_memory这一概念，也就是线程块的作用就在此发挥了。



![1738594370186](D:\fpga\cuda\cudastep\lesson8\figure\1738594370186.png)

共享内存是CUDA加速的核心思路。

共享内存的特点就是就是同一个线程块里的线程对其的访问速度快。

所以在设计算法时，我们要充分考虑这种同一线程块的关联特点。

比如在计算向量求和这一操作时，我们可以让一个线程块去共享一块内存，这个线程块就是相当于要处理的那个部分。

这样就从每次都从**显存里拿东西，进化到从SM里拿东西了**。

但是在使用共享时，有一个知识点叫做**bank**。我们说过，受限于GPU的硬件限制，线程必须一32为一WRAP的线程束进行执行，然后一个SM最多128个线程，但是受限于SM里的寄存器，共享内存等大小限制。同时执行的往往小于4个线程束，因此我们需要设计的东西就是。**合理规划这些限制因素，尽可能地让SM都拉满。**

bank直译就是**银行**，他是共享内存的组成方式，就是说，一个共享内存会被划分为若干个bank。一个bank是32字节。

A6000的一个SM的共享内存是128KB，也就是说，假设我们想把这个SM128线程的利用率拉满，就需要放4个wrap。也就是说我们的的线程块的大小，至少也要是warp的 1 倍 2倍 4倍。

比如线程

块大小32。 这样四个线程块占满一个SM。

线程块大小64。 2个线程块沾满一个SM。

但如果线程块大小为96。则一个SM只能放一个线程块了。因为不同SM的线程是不能利用SM的共享内存进行交互的。



现在我们理解了共享内存的运行方式，现在再细一点。 一个BANK32字节 也就是32B

为什么share_memory要划分成若干个bank呢？？？BANK名字叫银行，也就是他只能同时被一个线程所访问。

比如128KB的共享内存，线程块大小为32。不考虑寄存器不够用的情况下。一个SM里塞了四个线程块同时使用，所以一个线程块有32KB的共享内存。这一个线程块也就有了1024个BANK。

如果多个线程访问同一个银行中的数据，就会发生银行冲突，这会导致访问延迟。 

所以我们在开辟共享内存时有以下要求。

内存的索引方式，也就是我们取地址的方式，也就是我们设计数据结构时所占的空间大小。比如：

我设计的原子数据结构式他的他小是28B，一个int4B放了原子序数，3个double放坐标，这就是一种很失败的设计。因此他不能沾满一个bank，一个bank中既有第一个原子又有第二个原子，访问时会出现两个线程访问同一个bank的情况。所以需要合理规划共享内存的组织方式。



**L1缓存**

现在我们介绍了**自己可以动手组织的方式**，但是对于我们一直唾弃的global_memory。

他的访问速度特别慢，因此设计师借鉴了计算机中的cache理念设计了L1缓存，



是的，L1 缓存（一级缓存）可以理解为 GPU 的缓存之一，它的主要目的是加速对全局内存（global memory）的访问。

1. **L1 缓存的定义**

- **L1 缓存**：L1 缓存是 GPU 中离处理单元最近的缓存，通常用于存储最近访问的数据和指令，以提高访问速度。
- **层次结构**：在 GPU 中，L1 缓存位于 L2 缓存和全局内存之间，起到中介的作用。

2. **L1 缓存的功能**

- **加速访问**：L1 缓存的主要功能是减少访问全局内存的延迟。由于全局内存的访问延迟较高，L1 缓存可以存储热点数据，从而加快访问速度。
- **数据预取**：在访问全局内存时，L1 缓存通常会使用数据预取机制。即当访问某个地址时，后续的若干个字节（通常是一个缓存行的大小）也会被加载到 L1 缓存中，以便后续的访问能够更快速。

3. **L1 缓存的工作原理**

- **缓存一致性**：L1 缓存维护一定的缓存一致性，以确保处理单元读取的数据是最新的。
- **替换策略**：当 L1 缓存满时，使用特定的替换策略（如 LRU，最近最少使用）来决定哪些数据被替换。

4. **在 CUDA 中的应用**

- **优化内存访问**：程序设计中可以通过优化内存访问模式，尽量使得访问的地址相近，从而提高 L1 缓存的命中率，进一步加速计算。
- **使用共享内存**：共享内存和 L1 缓存可以结合使用，以进一步提高数据访问速度。

L1 缓存是 GPU 中的重要组成部分，它通过缓存最近访问的数据来加速对全局内存的访问。通过有效利用 L1 缓存，可以显著提高 CUDA 程序的性能。

在现代 GPU 中，缓存行的大小通常为 **128 字节**。这意味着当从全局内存中加载一个地址时，通常会同时加载后续的 128 字节到缓存中。这种设计有助于提高数据访问的效率，尤其是在访问连续内存地址时。 

也就是说，真的访问全局内存时，要遵循连续访问的原则，尽可能的利用到L1缓存。





现在全部理清了！！继续下一步

